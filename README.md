# Neural-Network-from-scratch
This is a simple neural network with input layer and ten neurons at the hidden layer and ten neurons at the output layer for classification of digits
from the MNIST dataset.
Activation for hidden layer is ReLu and activation for output layer is Softmax.
Batch Gradient boosting and Momentum based Gradient boosting has been done.
show_prediction function shows the actual label, predicted label and the digit image from the given input.
We can customize the input parameters, so that it can be used for different number of neurons at the hidden and output layers.
